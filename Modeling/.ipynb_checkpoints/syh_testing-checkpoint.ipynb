{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "opponent-crack",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn.model_selection\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import preprocessing\n",
    "from collections import defaultdict, Counter\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn import datasets, linear_model,cluster\n",
    "from sklearn.metrics import mean_squared_error, r2_score "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "seasonal-kenya",
   "metadata": {},
   "source": [
    "### Raw Data Reading "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "lovely-receptor",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw data has: 98078 rows\n"
     ]
    }
   ],
   "source": [
    "raw_df = pd.read_csv(\"zip_housing.csv\")\n",
    "print(\"Raw data has: \" + str(len(raw_df)) + \" rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "peaceful-stations",
   "metadata": {},
   "source": [
    "#### Raw Data Processing: Drop all rows where (beds, baths_full, lot_size, building_size) has na, and fill baths_half with 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "medium-stack",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped data has: 89200 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\python3\\lib\\site-packages\\pandas\\core\\frame.py:3188: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self[k1] = value[k2]\n"
     ]
    }
   ],
   "source": [
    "dropped_df = raw_df.dropna(subset=['beds','baths_full']) #,'lot_size','building_size'\n",
    "# Deleting unreasonable data\n",
    "# dropped_df= dropped_df[dropped_df['building_size'] >= 100]\n",
    "# dropped_df= dropped_df[dropped_df['lot_size'] >= 100]\n",
    "dropped_df[['baths_half']] = dropped_df[['baths_half']].fillna(0)\n",
    "dropped_df[['garage']] = dropped_df[['garage']].fillna(0)\n",
    "count = 0\n",
    "df = pd.DataFrame()\n",
    "for name, group in dropped_df:\n",
    "    group[[\"lot_size\"]].fillna(group.median(), inplace = True)\n",
    "    group[[\"building_size\"]].fillna(group.median(), inplace = True)\n",
    "    group\n",
    "    if(count == 0):\n",
    "        df = group\n",
    "        count += 1\n",
    "    else:\n",
    "        df = pd.concat([df, group])\n",
    "    print(\"After: \" + str(len(group)))\n",
    "print(\"Dropped data has: \" + str(len(dropped_df)) + \" rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "piano-sympathy",
   "metadata": {},
   "source": [
    "Since our dataset is large enough, and we do not have much professional knowledge about property markets, we decided to ignore all rows that contain null values instead of filling in estimate values (mean or median)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "existing-screen",
   "metadata": {},
   "source": [
    "## DBSCAN Method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "protected-sunglasses",
   "metadata": {},
   "source": [
    "#### Normalizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "concrete-heading",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input contains NaN, infinity or a value too large for dtype('float64').",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-4ed941f61662>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mscaled_dataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdropped_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdropped_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m11\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mscaled_dataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpreprocessing\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscaled_dataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mscaled_dataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscaled_dataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mscaled_dataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\python3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[1;31m# extra_args > 0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\python3\\lib\\site-packages\\sklearn\\preprocessing\\_data.py\u001b[0m in \u001b[0;36mnormalize\u001b[1;34m(X, norm, axis, copy, return_norm)\u001b[0m\n\u001b[0;32m   1902\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"'%d' is not a supported axis\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1903\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1904\u001b[1;33m     X = check_array(X, accept_sparse=sparse_format, copy=copy,\n\u001b[0m\u001b[0;32m   1905\u001b[0m                     estimator='the normalize function', dtype=FLOAT_DTYPES)\n\u001b[0;32m   1906\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0maxis\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\python3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[1;31m# extra_args > 0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\python3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[0;32m    661\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    662\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 663\u001b[1;33m             _assert_all_finite(array,\n\u001b[0m\u001b[0;32m    664\u001b[0m                                allow_nan=force_all_finite == 'allow-nan')\n\u001b[0;32m    665\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\python3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[1;34m(X, allow_nan, msg_dtype)\u001b[0m\n\u001b[0;32m    101\u001b[0m                 not allow_nan and not np.isfinite(X).all()):\n\u001b[0;32m    102\u001b[0m             \u001b[0mtype_err\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'infinity'\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mallow_nan\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;34m'NaN, infinity'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 103\u001b[1;33m             raise ValueError(\n\u001b[0m\u001b[0;32m    104\u001b[0m                     \u001b[0mmsg_err\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    105\u001b[0m                     (type_err,\n",
      "\u001b[1;31mValueError\u001b[0m: Input contains NaN, infinity or a value too large for dtype('float64')."
     ]
    }
   ],
   "source": [
    "scaled_dataset = dropped_df[dropped_df.columns[4:11]]\n",
    "scaled_dataset = preprocessing.normalize(scaled_dataset)\n",
    "scaled_dataset = pd.DataFrame(scaled_dataset)\n",
    "scaled_dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "advanced-fifteen",
   "metadata": {},
   "source": [
    "#### Finding Best DBSCAN (eps and Minpoints)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "colored-corpus",
   "metadata": {},
   "source": [
    "Minpoints is suggested as 2*dimension which should be 14 in our case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unsigned-firewall",
   "metadata": {},
   "outputs": [],
   "source": [
    "neighbors = NearestNeighbors(n_neighbors = 14)\n",
    "neighbors_fit = neighbors.fit(scaled_dataset)\n",
    "distances, indices = neighbors_fit.kneighbors(scaled_dataset)\n",
    "distances = np.sort(distances, axis=0)\n",
    "distances = distances[:,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "handled-delhi",
   "metadata": {},
   "source": [
    "#### Zoom in to find 'elbow' optimization point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "victorian-program",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(distances)\n",
    "plt.axis([74000, 80000, 0.0002, 0.0010])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faced-administration",
   "metadata": {},
   "source": [
    "#### Fit DBSCAN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "anticipated-knock",
   "metadata": {},
   "outputs": [],
   "source": [
    "minpts = 14\n",
    "e = 0.0006\n",
    "db = cluster.DBSCAN(eps=e,min_samples=minpts, metric='euclidean', \n",
    "                    metric_params=None, algorithm='auto', \n",
    "                    leaf_size=30, p=None, n_jobs=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "official-optimization",
   "metadata": {},
   "source": [
    "#### Get Labels: 0 for 'not outliers'; 1 for 'outliers'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "flying-tracy",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = db.fit(scaled_dataset)\n",
    "labels = []\n",
    "outliers = 0\n",
    "for i in model.labels_:\n",
    "    if i == -1:\n",
    "        labels.append(1)\n",
    "        outliers += 1\n",
    "    else:\n",
    "        labels.append(0)\n",
    "print(\"Found total: \" + str(outliers) + \" outliers.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "failing-overall",
   "metadata": {},
   "source": [
    "#### Combine Outliers and Processed Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interesting-filename",
   "metadata": {},
   "outputs": [],
   "source": [
    "dropped_df['outliers'] = labels\n",
    "noise_free_df = dropped_df[dropped_df['outliers'] == 0]\n",
    "print(\"Noise free data has: \" + str(len(noise_free_df)) + \" rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "infinite-authorization",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Highest Price: ' + str(max(noise_free_df['price'])))\n",
    "print('Largest Lot:   ' + str(max(noise_free_df['lot_size'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "smaller-colon",
   "metadata": {},
   "source": [
    "After all processing before, we still have the noise like (price == 915,000,000, or lot_size == 4,356,000,000) which we do not want in our processed dataset. The reason for this may because of the inner work of DBSCAN, which is unsupervised clustering. This raw dataset may contain enough data for those extrodinary large values to become an individual cluster. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "convinced-quantity",
   "metadata": {},
   "source": [
    "### End of DBSCAN Method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "trained-anime",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "identical-tracker",
   "metadata": {},
   "source": [
    "## IQR (interquantile range) Method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "european-guest",
   "metadata": {},
   "source": [
    "#### Find IQR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "consecutive-negative",
   "metadata": {},
   "outputs": [],
   "source": [
    "iqr_dataset = dropped_df[dropped_df.columns[4:11]]\n",
    "Q1 = iqr_dataset.quantile(0.25)\n",
    "Q3 = iqr_dataset.quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "print(IQR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "registered-tender",
   "metadata": {},
   "outputs": [],
   "source": [
    "lower = Q1 - 1.5 * IQR\n",
    "higer = Q3 + 1.5 * IQR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "recorded-thermal",
   "metadata": {},
   "outputs": [],
   "source": [
    "iqr_dataset_out = iqr_dataset[~((iqr_dataset < (lower)) |(iqr_dataset > (higer))).any(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "charming-courtesy",
   "metadata": {},
   "outputs": [],
   "source": [
    "iqr_df = raw_df.iloc[list(iqr_dataset_out.index),:]\n",
    "iqr_df[['baths_half']] = iqr_df[['baths_half']].fillna(0)\n",
    "iqr_df[['garage']] = iqr_df[['garage']].fillna(0)\n",
    "print(\"IQR data has: \" + str(len(iqr_df)) + \" rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "international-polish",
   "metadata": {},
   "source": [
    "### Result Graphs for IQR (Blue is original data; Orange is processed data) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "finnish-boating",
   "metadata": {},
   "source": [
    "#### Building size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "macro-majority",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, 1)\n",
    "axs[0].plot(np.sort(dropped_df['building_size']))\n",
    "axs[1].plot(np.sort(iqr_dataset_out['building_size']), 'tab:orange')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cardiovascular-causing",
   "metadata": {},
   "source": [
    "#### Price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "innocent-ballot",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, 1)\n",
    "axs[0].plot(np.sort(dropped_df['price']))\n",
    "axs[1].plot(np.sort(iqr_dataset_out['price']), 'tab:orange')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "english-circular",
   "metadata": {},
   "source": [
    "#### IQR method is considering all data as a whole. It is not based on any kind of clustering. Although we eliniminated a lot of extrodinary large values, we also eliniminated some reasonable data only because it is larger than the 75% quantile (price). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adapted-print",
   "metadata": {},
   "source": [
    "### End of IQR method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "behind-colleague",
   "metadata": {},
   "source": [
    "### Overall, I think the data processed by IQR method is much more reasonable than the DBSCAN method, but either case need to be further adjusted to fulfill our estimatation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
